{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is modified from Krish Naik's YouTube [video](https://www.youtube.com/watch?v=hH4WkgILUD4) for Harvard's GenEd 1188 on Generative AI. \n",
    "\n",
    "The purpose of this tutorial is for designing a potential technical section activity for future students of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: LlamaIndex is constantly developing. They seem to change the structure of the library quite often.\n",
    "#        If the imports are not successful, please google and find the newest structure to import the\n",
    "#        necessary libraries.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the environment is loaded\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that we stored our OpenAI API key in the .env file and named it \"OPENAI_API_KEY\"\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Our PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the metadata of our documents\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c6036c540848d5b925bca030e02441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44128c970a034d2badcdf99ceb8e0fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the documents to an index\n",
    "# from this index, we can directly query any questions we have\n",
    "index=VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Query Engine to Retrieve Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the query_engine is responsible for retrieving information from the indexes\n",
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer is a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output. It eschews recurrence and instead uses self-attention to compute representations of its input and output without relying on sequence-aligned recurrent neural networks or convolutional layers.\n"
     ]
    }
   ],
   "source": [
    "response1=query_engine.query(\"What is a transformer\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO is a new approach to object detection that frames the task as a regression problem to spatially separated bounding boxes and associated class probabilities. It uses a single neural network to predict bounding boxes and class probabilities directly from full images in one evaluation. YOLO is known for its speed, processing images in real-time and achieving high mean average precision compared to other real-time detection systems.\n"
     ]
    }
   ],
   "source": [
    "response2=query_engine.query(\"What is YOLO\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: The Transformer is a model architecture that relies\n",
      "entirely on an attention mechanism to draw global dependencies between\n",
      "input and output. It eschews recurrence and instead uses self-\n",
      "attention to compute representations of its input and output without\n",
      "relying on sequence-aligned recurrent neural networks or convolutional\n",
      "layers.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 2e2aac14-82ff-4f91-8a8f-2c499a8379b1\n",
      "Similarity: 0.7885361023202706\n",
      "Text: Table 4: The Transformer generalizes well to English\n",
      "constituency parsing (Results are on Section 23 of WSJ) Parser\n",
      "Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only,\n",
      "discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative\n",
      "90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al.\n",
      "(2016) [8] WSJ only, disc...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 200522ec-d3f8-425f-badc-156f381c6227\n",
      "Similarity: 0.7837820802208945\n",
      "Text: 1 Introduction Recurrent neural networks, long short-term memory\n",
      "[ 13] and gated recurrent [ 7] neural networks in particular, have\n",
      "been firmly established as state of the art approaches in sequence\n",
      "modeling and transduction problems such as language modeling and\n",
      "machine translation [ 35,2,5]. Numerous efforts have since continued\n",
      "to push the bo...\n",
      "====================\n",
      "Final Response: YOLO is a new approach to object detection that frames\n",
      "the task as a regression problem to spatially separated bounding boxes\n",
      "and associated class probabilities. It uses a single neural network to\n",
      "predict bounding boxes and class probabilities directly from full\n",
      "images in one evaluation. YOLO is known for its speed, processing\n",
      "images in real-time and achieving high mean average precision compared\n",
      "to other real-time detection systems.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: dc7c7f93-ed1b-45c6-be03-0d3056cf24d6\n",
      "Similarity: 0.8137150587504083\n",
      "Text: You Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University\n",
      "of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/ Abstract We present YOLO, a new approach to\n",
      "object detection. Prior work on object detection repurposes classiﬁers\n",
      "to per- form...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: c185be60-0eef-4596-b9db-ed875b7b031a\n",
      "Similarity: 0.8072749426586832\n",
      "Text: YOLO is refreshingly simple: see Figure 1. A sin- gle\n",
      "convolutional network simultaneously predicts multi- ple bounding\n",
      "boxes and class probabilities for those boxes. YOLO trains on full\n",
      "images and directly optimizes detec- tion performance. This uniﬁed\n",
      "model has several beneﬁts over traditional methods of object\n",
      "detection. First, YOLO is extrem...\n"
     ]
    }
   ],
   "source": [
    "# use `pprint_response` for much better presentation of the results\n",
    "# `pprint_response` is powerful because it will show you the best response along with similarity scores with the source\n",
    "pprint_response(response1, show_source=True)\n",
    "print(\"====================\")\n",
    "pprint_response(response2, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Advanced Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also retrieve more than 1 results by setting the number of k\n",
    "retriever=VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "query_engine_advanced=RetrieverQueryEngine(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: A transformer is a model architecture that relies\n",
      "entirely on attention mechanisms to draw global dependencies between\n",
      "input and output, without using recurrent layers or convolutions. It\n",
      "allows for more parallelization and has been shown to achieve state-\n",
      "of-the-art results in tasks like machine translation.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 2e2aac14-82ff-4f91-8a8f-2c499a8379b1\n",
      "Similarity: 0.7885361023202706\n",
      "Text: Table 4: The Transformer generalizes well to English\n",
      "constituency parsing (Results are on Section 23 of WSJ) Parser\n",
      "Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only,\n",
      "discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative\n",
      "90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al.\n",
      "(2016) [8] WSJ only, disc...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 200522ec-d3f8-425f-badc-156f381c6227\n",
      "Similarity: 0.7837820802208945\n",
      "Text: 1 Introduction Recurrent neural networks, long short-term memory\n",
      "[ 13] and gated recurrent [ 7] neural networks in particular, have\n",
      "been firmly established as state of the art approaches in sequence\n",
      "modeling and transduction problems such as language modeling and\n",
      "machine translation [ 35,2,5]. Numerous efforts have since continued\n",
      "to push the bo...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 6fa12ffb-f750-40ce-955f-7c7ccf2f4f9d\n",
      "Similarity: 0.7827210198383343\n",
      "Text: Figure 1: The Transformer - model architecture. The Transformer\n",
      "follows this overall architecture using stacked self-attention and\n",
      "point-wise, fully connected layers for both the encoder and decoder,\n",
      "shown in the left and right halves of Figure 1, respectively. 3.1\n",
      "Encoder and Decoder Stacks Encoder: The encoder is composed of a stack\n",
      "of N= 6 id...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: d4447894-042f-47d0-9993-d694eff62378\n",
      "Similarity: 0.7784087703000329\n",
      "Text: Provided proper attribution is provided, Google hereby grants\n",
      "permission to reproduce the tables and figures in this paper solely\n",
      "for use in journalistic or scholarly works. Attention Is All You Need\n",
      "Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google\n",
      "Brain noam@google.comNiki Parmar∗ Google Research\n",
      "nikip@google.comJakob Uszkor...\n"
     ]
    }
   ],
   "source": [
    "response1_advanced = query_engine_advanced.query(\"What is a transformer\")\n",
    "pprint_response(response1_advanced, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO is a new approach to object detection that frames\n",
      "the task as a regression problem to spatially separated bounding boxes\n",
      "and associated class probabilities. It uses a single neural network to\n",
      "predict bounding boxes and class probabilities directly from full\n",
      "images in one evaluation. YOLO is known for its speed, processing\n",
      "images in real-time at high frame rates while achieving good mean\n",
      "average precision compared to other real-time detectors. Additionally,\n",
      "YOLO is designed to reason globally about the image and can be trained\n",
      "directly on full images, offering a simple and unified model for\n",
      "object detection.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: dc7c7f93-ed1b-45c6-be03-0d3056cf24d6\n",
      "Similarity: 0.8137150587504083\n",
      "Text: You Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University\n",
      "of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/ Abstract We present YOLO, a new approach to\n",
      "object detection. Prior work on object detection repurposes classiﬁers\n",
      "to per- form...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: c185be60-0eef-4596-b9db-ed875b7b031a\n",
      "Similarity: 0.8072749426586832\n",
      "Text: YOLO is refreshingly simple: see Figure 1. A sin- gle\n",
      "convolutional network simultaneously predicts multi- ple bounding\n",
      "boxes and class probabilities for those boxes. YOLO trains on full\n",
      "images and directly optimizes detec- tion performance. This uniﬁed\n",
      "model has several beneﬁts over traditional methods of object\n",
      "detection. First, YOLO is extrem...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 05eca526-03f2-445e-b10e-123a6a9c2f3f\n",
      "Similarity: 0.8031388655402845\n",
      "Text: Poselets RCNN D&THumans DPMYOLO (a)Picasso Dataset precision-\n",
      "recall curves.VOC 2007 Picasso People-Art AP AP BestF1 AP YOLO 59.2\n",
      "53.3 0.590 45 R-CNN 54.2 10.4 0.226 26 DPM 43.2 37.8 0.458 32 Poselets\n",
      "[2] 36.5 17.8 0.271 D&T [4] - 1.9 0.051 (b)Quantitative results on the\n",
      "VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset\n",
      "evaluates on...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 6c05c7e6-a0c6-4f16-b17d-824834e9dd49\n",
      "Similarity: 0.7894324276751336\n",
      "Text: 8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3\n",
      "68.3 57.8 29.6 57.8 40.9 59.3 54.1 SDS [16] 50.7 69.7 58.4 48.5 28.3\n",
      "28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6\n",
      "58.9 50.7 R-CNN [13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5\n",
      "48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6 Table 3: PASCAL\n",
      "VOC ...\n"
     ]
    }
   ],
   "source": [
    "response2_advanced = query_engine_advanced.query(\"What is YOLO\")\n",
    "pprint_response(response2_advanced, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also filter responses to display by similarity scores (e.g. only show responses with similarity score > 0.8)\n",
    "postprocessor=SimilarityPostprocessor(similarity_cutoff=0.8)\n",
    "query_engine_advanced2=RetrieverQueryEngine(retriever=retriever,\n",
    "                                            node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Empty Response\n"
     ]
    }
   ],
   "source": [
    "response1_advanced2 = query_engine_advanced2.query(\"What is a transformer\")\n",
    "pprint_response(response1_advanced2, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO is a new approach to object detection that frames\n",
      "the task as a regression problem to spatially separated bounding boxes\n",
      "and associated class probabilities. It uses a single neural network to\n",
      "predict bounding boxes and class probabilities directly from full\n",
      "images in one evaluation, optimizing end-to-end directly on detection\n",
      "performance. YOLO is known for its speed, processing images in real-\n",
      "time and achieving high mean average precision compared to other real-\n",
      "time systems.\n",
      "______________________________________________________________________\n",
      "Source Node 1/3\n",
      "Node ID: dc7c7f93-ed1b-45c6-be03-0d3056cf24d6\n",
      "Similarity: 0.8137150587504083\n",
      "Text: You Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University\n",
      "of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/ Abstract We present YOLO, a new approach to\n",
      "object detection. Prior work on object detection repurposes classiﬁers\n",
      "to per- form...\n",
      "______________________________________________________________________\n",
      "Source Node 2/3\n",
      "Node ID: c185be60-0eef-4596-b9db-ed875b7b031a\n",
      "Similarity: 0.8072749426586832\n",
      "Text: YOLO is refreshingly simple: see Figure 1. A sin- gle\n",
      "convolutional network simultaneously predicts multi- ple bounding\n",
      "boxes and class probabilities for those boxes. YOLO trains on full\n",
      "images and directly optimizes detec- tion performance. This uniﬁed\n",
      "model has several beneﬁts over traditional methods of object\n",
      "detection. First, YOLO is extrem...\n",
      "______________________________________________________________________\n",
      "Source Node 3/3\n",
      "Node ID: 05eca526-03f2-445e-b10e-123a6a9c2f3f\n",
      "Similarity: 0.8031388655402845\n",
      "Text: Poselets RCNN D&THumans DPMYOLO (a)Picasso Dataset precision-\n",
      "recall curves.VOC 2007 Picasso People-Art AP AP BestF1 AP YOLO 59.2\n",
      "53.3 0.590 45 R-CNN 54.2 10.4 0.226 26 DPM 43.2 37.8 0.458 32 Poselets\n",
      "[2] 36.5 17.8 0.271 D&T [4] - 1.9 0.051 (b)Quantitative results on the\n",
      "VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset\n",
      "evaluates on...\n"
     ]
    }
   ],
   "source": [
    "response2_advanced2 = query_engine_advanced2.query(\"What is YOLO\")\n",
    "pprint_response(response2_advanced2, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results for Future Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've run the following cell, you should see a new folder called `storage` which contains the index results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a model architecture that relies entirely on an attention mechanism to establish global dependencies between input and output, without utilizing recurrent layers commonly found in encoder-decoder structures. This approach allows for increased parallelization during training and has shown significant improvements in translation quality, surpassing previous state-of-the-art models in certain tasks.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are transformers?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just finished building a basic RAG system! To be honest, it is very similar to what ChatGPT4 can achieve today. But having LlamaIndex and knowing how RAG works is helpful as they show you how people are trying to solve the problems around LLMs and provide you with a framework for solving these kinds of problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
